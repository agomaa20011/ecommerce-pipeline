# E-Commerce Data Pipeline #

## ğŸ¯ Overview ##

This project demonstrates a modern end-to-end data pipeline for e-commerce analytics.
It extracts raw event data, transforms it with Apache Spark, loads it into PostgreSQL, models it using dbt, and visualizes business insights with Apache Superset.

## ğŸ§± Project Architecture ##

```
Raw CSV (Landing) 
   â†“
Bronze Layer (Spark)
   â†“
PostgreSQL (Staging)
   â†“
dbt (Silver & Gold models)
   â†“
Superset Dashboard (Visualization)
```

![flow diagram](https://github.com/agomaa20011/ecommerce-pipeline/blob/main/pipeline%20Arch.drawio.png)


## âš™ï¸ Technologies Used ##

| Tool | Purpose |
|------|----------|
| ğŸ **Python** | Core programming language used for scripting and automation |
| ğŸ”¥ **Apache Spark** | Data cleaning, transformation, and Parquet file generation |
| ğŸ˜ **PostgreSQL** | Central data warehouse for structured storage |
| ğŸ§± **dbt** | Data modeling and schema management (staging â†’ silver â†’ gold) |
| ğŸ“Š **Apache Superset** | Data visualization and interactive dashboards |
| ğŸ™ **GitHub** | Version control and project collaboration |

## ğŸ§© Data Architecture Layers ##

### ğŸŸ¤ Bronze Layer (Raw â†’ Cleaned Parquet) ###
	â€¢	Tool: Apache Spark
	â€¢	Input: CSV files (data/landing/)
	â€¢	Output: Cleaned, partitioned Parquet files (data/bronze/events/)
	â€¢	Main script: bronze_transform.py

Key operations:
	â€¢	Lowercasing text fields (brand, category_code)
	â€¢	Parsing timestamps
	â€¢	Adding ingestion metadata (ingested_at, event_date)

### ğŸŸ  Staging Layer (stg in PostgreSQL) ###

![stg](https://github.com/agomaa20011/ecommerce-pipeline/blob/main/data%20warehous%20stg.drawio.png)

	â€¢	Table: stg.events
	â€¢	Created by: load_to_postgres.py
	â€¢	Purpose: Store typed, validated event data directly from the bronze layer before business transformations.

### âšª Silver Layer (silver schema) ###

![silver](https://github.com/agomaa20011/ecommerce-pipeline/blob/main/data%20warehous%20silver.drawio%20(1).png)

Purpose: Transform raw events into a star schema for analytical modeling.

Relationships:
dim_users (1)â”€â”€â”€< fact_events >â”€â”€â”€(1) dim_products

### ğŸŸ¡ Gold Layer (analytics schema via dbt) ###

Purpose: Create business-ready metrics and aggregations for the dashboard.

| Model | Description |
| ------------- | ------------- |
| analytics.daily_conversion  | Daily conversion rates by date |
| analytics.hourly_activity | Activity trends per hour |
| analytics.session_metrics | Session-level KPIs |
| analytics.top_products | Top-selling and viewed products |
| analytics.category_price_trends  | Price trends per category |

## ğŸ“‚ Project Structure ##

```
ecommerce-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ landing/      # raw CSVs (not uploaded)
â”‚   â”œâ”€â”€ bronze/       # parquet files (not uploaded)
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â”‚   â”œâ”€â”€ daily_conversion.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ hourly_activity.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ session_metrics.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ top_products.sql
â”‚   â”‚   â”‚   â””â”€â”€ category_price_trends.sql
â”‚   â”œâ”€â”€ schema.yml
â”‚   â”œâ”€â”€ source.yml
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ bronze_transform.py      # Spark transformation (landing â†’ bronze)
â”œâ”€â”€ load_to_postgres.py      # Load parquet â†’ PostgreSQL staging
â”œâ”€â”€ run_pipeline.sh          # Runs all steps end-to-end
â”œâ”€â”€ superset_config.py       # Superset configuration
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸš€ How to Run the Project ##

### 1ï¸âƒ£ Set up PostgreSQL ###
	1.	Install PostgreSQL (version 14 or newer).
    2.  2. Create a new database and user:
	sql
	CREATE DATABASE ecommerce_pipeline;
	CREATE USER postgres WITH PASSWORD '1234';
	GRANT ALL PRIVILEGES ON DATABASE ecommerce_pipeline TO postgres;

### 2ï¸âƒ£ Install dependencies ###
```
pip install -r requirements.txt
```

### 3ï¸âƒ£ Prepare directories ###
```
data/
 â”œâ”€â”€ landing/  â† place your CSV files here
 â””â”€â”€ bronze/
```
 ### 4ï¸âƒ£ Run the full pipeline ###
 ```
 bash run_pipeline.sh
 ```

 This will:
	â€¢	Read & clean CSV data (Spark)
	â€¢	Write Parquet files
	â€¢	Load them into PostgreSQL
	â€¢	Run dbt models & tests

### 5ï¸âƒ£ Launch Superset dashboard ###
```
superset run -p 8088 --host 127.0.0.1
```

Then open:
```
http://localhost:8088
```

## ğŸ“Š Dashboard Metrics ##
The dashboard includes:
â€¢ Top Products by Revenue
â€¢ Daily Conversion Rates
â€¢ Session Metrics
â€¢ Category Price Trends
â€¢ Hourly User Activity

![dashboard](https://github.com/agomaa20011/ecommerce-pipeline/blob/main/Screenshot%202025-10-16%20at%2020.59.01.png)
![dashboard](https://github.com/agomaa20011/ecommerce-pipeline/blob/main/Screenshot%202025-10-16%20at%2020.59.28.png)



### ğŸ’¡ Notes ###
* The raw dataset is not uploaded due to size limits.You can download it from [Kaggle](https://www.kaggle.com/datasets/yashtyagi1712/ecommercebehaviordatafrommulticategorystore).
* Database connection details are defined in profiles.yml.
* To rebuild the dashboard, connect Superset to your PostgreSQL database and import the SQL models.


---

ğŸ‘¤ **Author:** Ahmed Abohamad  




